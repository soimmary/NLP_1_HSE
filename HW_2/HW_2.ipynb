{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW_2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой домашке вам будет нужно найти тексты на русском языке, в которых будут какие-то трудные или неоднозначные для POS теггинга моменты и разметить их вручную – с помощью этих текстов мы будем оценивать качество работы наших теггеров. В текстах размечаем только части речи, ничего больше!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WARNING\n",
    "**Я дурочка с переулочка, невнимательно прочитала задание, поэтому размечала не только POS, но и еще род, число, время и т. п.**\n",
    "\n",
    "**Поэтому у меня для 3 пункта будет 2 функции: одна – по заданию – для подсчета accuracy pos, а другая – расширенная – для сравнения всех тегов (время, род, число и т. п.)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ✅ (1 балл) Создание, разметка корпуса и объяснение того, почему этот текст подходит для оценки (какие моменты вы тут считаете трудными для автоматического посттеггинга и почему). Не забывайте, что разные теггеры могут использовать разные тегсеты: напишите комментарий о том, какой тегсет вы берёте для разметки и почему.\n",
    "- ✅ (3 балла) Потом вам будет нужно взять три POS теггера для русского языка (**udpipe**, **natasha**, **pymorphy**) и «прогнать» текст через каждый из них.\n",
    "- ✅ (2 балла) Затем оценим accuracy для каждого теггера. Заметьте, что в разных системах имена тегов и части речи могут отличаться, – вам надо будет свести это всё к единому стандарту с помощью какой-то функции-конвертера и сравнить с вашим размеченным руками эталоном - тоже с помощью какого-то кода или функции.\n",
    "- (2 балла) Дальше вам нужно взять лучший теггер для русского языка и с его помощью написать функцию (chunker), которая выделяет из размеченного текста 3 типа n-грамм, соответствующих какому-то шаблону (к примеру не + какая-то часть речи или NP или сущ.+ наречие и тд)\n",
    "- (2-3 балла) В предыдущем дз многие из вас справедливо заметили, что если бы мы могли класть в словарь не только отдельные слова, но и словосочетания, то программа работала бы лучше. Предложите 3 шаблона (слово + POS-тег / POS-тег + POS-тег) запись которых в словарь, по вашему мнению, улучшила бы качество работы программы из предыдущей домашки. Балл за объяснение того, почему именно эти группы вы взяли, балл за встраивание функции в программу из предыдущей домашки, балл за сравнение качества предсказания тональности с улучшением и без (это бонусный одиннадцатый балл)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from corpy.udpipe import Model, load, dump\n",
    "from razdel import sentenize, tokenize\n",
    "from slovnet import Morph\n",
    "from navec import Navec\n",
    "import pymorphy2\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "pymorph = pymorphy2.MorphAnalyzer()\n",
    "model = Model('russian-syntagrus-ud-2.5-191206.udpipe')\n",
    "punctuation = \"\"\"!\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~–—«»\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Создание и разметка корпуса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.google.com/spreadsheets/d/1MdvFcSXRpxD0bcRKDRRP3WSEyUaoXFpWiOFkbpo2j-c/edit#gid=825959713"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Почему этот текст подходит для оценки?**\n",
    "1. Продуктивные модели: _шиммерный_.\n",
    "2. Сложные слова: _фан-клуб, лос-анджелесский, апсайкл-деним, утракороткий_.\n",
    "3. Слова с «неизвестными» корнями без продуктивных аффиксов: _краш, ресейл, инстаграм, апсайклинг, мюли (можно принять за глагол прош. вр. во мн. ч.)_.\n",
    "4. Редкие и нестандартные формы: _постя_.\n",
    "5. Куча именованных сущностей: _Эмили Адамс Боде, Жан-Поль Готье_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Кроп-топ Жан-Поль Готье, сумочка Шанель — Беллу Хадид смело можно назвать королевой винтажа.\n",
    "Но осознанным покупателем ее делает не только пристрастие к ресейлу (помните, надевая \n",
    "что-то винтажное, вы сокращаете углеродный след своего гардероба, а еще продлеваете \n",
    "жизнь конкретной вещи) — модель постоянно поддерживает молодые независимые марки, \n",
    "которые практикуют устойчивость и экологичность, постя фотографии в Инстаграм. \n",
    "В фан-клуб Эмили Адамс Боде Белла Хадид вступила в марте 2020-го в самом начале карантина — \n",
    "именно тогда в ее инстаграме появилась серия фотографий под кодовым названием «любимый \n",
    "желтый жакет». Эмили известна своей страстью к апсайклингу и винтажной стилистике, \n",
    "так что их союзу с Беллой удивляться не приходится. В декабре последняя была замечена \n",
    "в джемпере Виктория и броских брюках Соуллэнд — очередное подтверждение того, что \n",
    "экологичные творения Боде могут быть очень универсальными. Еще один карантинный краш Хадид — \n",
    "это расписанные вручную брюки лос-анджелесской художницы Джульет Джонстоун. \n",
    "Джонстоун использует старые пары как «холст». Результат — по-настоящему уникальные \n",
    "вещи с очень экологичной репутацией. Эстетика нулевых буквально заполонила ленту инстаграма: \n",
    "ультракороткие юбки, мюли, шиммерные тени, глиттер. Белла — большая поклонница всего \n",
    "винтажного — просто не могла не полюбить Редан (бренд, который обновляет и перепродает \n",
    "старые джинсы Левайс). Казалось бы, куда осознаннее, но Редан пошли еще дальше — запустили \n",
    "у себя на сайте секцию ресейла. Теперь их апсайкл-деним будет жить и носиться вечно.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Тегсет**\n",
    "\n",
    "Выбран на основе статьи \"NLP evaluation: russian morphological parsers\".\n",
    "Морфологические признаки указываются только для существительных, глаголов и прилагательных.\n",
    "\n",
    "- **род**: m (мужской), f (женский), n (средний).\n",
    "- **падеж**: nom (именительный), gen (родитель- ный, в том числе счетная форма — два шар/а), dat (дательный), acc (винительный), ins (твори- тельный), loc (предложный, в том числе второй предложный, ср. в лесу).\n",
    "- **число**: sg (единственное), pl (множественное).\n",
    "- **время**: pres (= непрошедшее: настоящее и будущее время — пишу, напишу), past (прошедшее).\n",
    "- **наклонение**: imper (повелительное).\n",
    "- **инфинитив**: inf.\n",
    "- **причастие**: partcp.\n",
    "- **деепричастие**: ger.\n",
    "- **залог**: act (действительный), pass (страдательный) — указывается только в формах причастий.\n",
    "- **лицо**: 1p, 2p, 3p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return [i for i in nltk.word_tokenize(text) if i not in punctuation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаю токены в csv, чтобы разметить вручную. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('HW2_annotated_text.csv', 'a', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=';')\n",
    "    writer.writerow(['token', 'lemma', 'pos', 'gender', \n",
    "                      'case', 'num', 'tense', 'mood', 'inf', \n",
    "                      'partcp', 'ger', 'voice', 'person'])\n",
    "    for token in tokenizer(text):\n",
    "        writer.writerow([token,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAQ\n",
    "\n",
    "- Как оценивать accuracy, если POS-теггер не разрешает омонимию? - Проверить, что это точно так. За правильные ответы тогда можно считать случаи, когда тег из gold-разметки есть в ответе теггера."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UDPipe: https://corpy.readthedocs.io/en/stable/guides/udpipe.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Тестируем POS-теггеры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pymorphy_analysis(text):\n",
    "    pymorphy_dict = defaultdict()\n",
    "    tokens = tokenizer(text)\n",
    "    \n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        p = pymorph.parse(token)[0]\n",
    "        pymorphy_dict[token] = {}\n",
    "        \n",
    "        if p.tag.POS in ['NOUN', 'VERB']:\n",
    "            pymorphy_dict[token]['POS'] = p.tag.POS\n",
    "            \n",
    "        elif p.tag.POS == 'INFN':\n",
    "            pymorphy_dict[token]['POS'] = 'VERB'\n",
    "            pymorphy_dict[token]['inf'] = 'inf'\n",
    "\n",
    "        elif p.tag.POS in ['PRTF', 'PRTS']:\n",
    "            pymorphy_dict[token]['POS'] = 'VERB'\n",
    "            pymorphy_dict[token]['partcp'] = 'partcp'\n",
    "            \n",
    "        elif p.tag.POS in ['ADJF', 'ADJS']:\n",
    "            pymorphy_dict[token]['POS'] = 'ADJ'\n",
    "        \n",
    "        elif p.tag.POS == 'GRND':\n",
    "            pymorphy_dict[token]['POS'] = 'VERB'\n",
    "            pymorphy_dict[token]['ger'] = 'ger'\n",
    "            \n",
    "        else:\n",
    "            del pymorphy_dict[token]\n",
    "            continue\n",
    "        \n",
    "        #pymorphy_dict[token]['lemma'] = pymorph.parse(token)[0].normal_form\n",
    "        pymorphy_dict[token]['gender'] = p.tag.gender\n",
    "        pymorphy_dict[token]['case'] = p.tag.case      \n",
    "        pymorphy_dict[token]['num'] = p.tag.number \n",
    "        pymorphy_dict[token]['tense'] = p.tag.tense\n",
    "        pymorphy_dict[token]['mood'] = p.tag.mood\n",
    "        pymorphy_dict[token]['voice'] = p.tag.voice\n",
    "        pymorphy_dict[token]['person'] = p.tag.person\n",
    "        \n",
    "    return pymorphy_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "navec = Navec.load('navec_news_v1_1B_250K_300d_100q.tar')\n",
    "morph = Morph.load('slovnet_morph_news_v1.tar', batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slovnet_analysis(text):\n",
    "    slovnet_dict = defaultdict()\n",
    "    \n",
    "    chunk = [[_.text for _ in tokenize(sent.text)] for sent in sentenize(text)]\n",
    "    morph.navec(navec)\n",
    "    for markup in morph.map(chunk):\n",
    "        for token in markup.tokens:\n",
    "            text = token.text.lower()\n",
    "            if token.pos not in ['ADJ', 'NOUN', 'VERB', 'PROPN']:\n",
    "                continue\n",
    "\n",
    "            slovnet_dict[text] = {}\n",
    "            \n",
    "            if token.pos == 'PROPN':\n",
    "                slovnet_dict[text]['POS'] = 'NOUN'\n",
    "            else:\n",
    "                slovnet_dict[text]['POS'] = token.pos\n",
    "            \n",
    "            feats = token.feats    \n",
    "            slovnet_dict[text]['gender'] = feats.get('Gender', None) \n",
    "            slovnet_dict[text]['case'] = feats.get('Case', None)\n",
    "            slovnet_dict[text]['num'] = feats.get('Number', None)\n",
    "            slovnet_dict[text]['tense'] = feats.get('Tense', None)\n",
    "            slovnet_dict[text]['mood'] = feats.get('Mood', None)\n",
    "            slovnet_dict[text]['voice'] = feats.get('Voice', None)\n",
    "            slovnet_dict[text]['person'] = feats.get('Person', None)\n",
    "            try:\n",
    "                if token.feats['VerbForm'] == 'Inf':\n",
    "                    slovnet_dict[text]['inf'] = 'inf'\n",
    "            except KeyError:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                if token.feats['VerbForm'] == 'Part':\n",
    "                    slovnet_dict[text]['partcp'] = 'partcp'\n",
    "            except KeyError:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                if token.feats['VerbForm'] == 'Conv':\n",
    "                    slovnet_dict[text]['ger'] = 'ger'\n",
    "            except KeyError:\n",
    "                pass\n",
    "            \n",
    "    return slovnet_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Udpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def udpipe_analysis(text):\n",
    "    udpipe_dict = defaultdict()\n",
    "    sentences = [s.split('\\t')[1:6] for s in \"\".join(dump(model.process(text))).split('\\n') \n",
    "                 if re.match(r'\\d', s) and s.split('\\t')[3] in ['NOUN', 'ADJ', 'VERB', 'PROPN']]\n",
    "    \n",
    "    for token_features in sentences:\n",
    "        token = token_features[0].lower()\n",
    "        udpipe_dict[token] = {}\n",
    "        gr_features_dict = {}\n",
    "        gr_features = token_features[4].split('|')\n",
    "        for feature in gr_features:\n",
    "            try:\n",
    "                gr_features_dict[feature.split('=')[0]] = feature.split('=')[1]\n",
    "            except IndexError:\n",
    "                continue\n",
    "            \n",
    "        udpipe_dict[token]['POS'] = token_features[2]\n",
    "        try:\n",
    "            udpipe_dict[token]['gender'] = gr_features_dict['Gender']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            udpipe_dict[token]['case'] = gr_features_dict['Case']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            udpipe_dict[token]['num'] = gr_features_dict['Number']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            udpipe_dict[token]['tense'] = gr_features_dict['Tense']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            udpipe_dict[token]['mood'] = gr_features_dict['Mood']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            udpipe_dict[token]['voice'] = gr_features_dict['Voice']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            udpipe_dict[token]['person'] = gr_features_dict['Person']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            udpipe_dict[token]['case'] = gr_features_dict['Case']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            if gr_features_dict['VerbForm'] == 'Part':\n",
    "                udpipe_dict[token]['partcp'] = 'partcp'\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            if gr_features_dict['VerbForm'] == 'Inf':\n",
    "                udpipe_dict[token]['inf'] = 'inf'\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            if gr_features_dict['VerbForm'] == 'Conv':\n",
    "                udpipe_dict[token]['ger'] = 'ger'\n",
    "        except KeyError:\n",
    "            pass\n",
    "   \n",
    "    return udpipe_dict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Золотой стандарт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_golden_standart(path='HW2_annotated_text.csv'):\n",
    "    golden_dict = defaultdict()\n",
    "    with open (path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        #next(reader, None)\n",
    "        for token_info in reader:\n",
    "            if token_info[2] in ['NOUN', 'VERB', 'ADJ']:\n",
    "                golden_dict[token_info[0]] = {}\n",
    "                golden_dict[token_info[0]]['POS'] = token_info[2]\n",
    "                if token_info[3]:\n",
    "                    golden_dict[token_info[0]]['gender'] = token_info[3]\n",
    "                if token_info[4]:\n",
    "                    golden_dict[token_info[0]]['case'] = token_info[4]\n",
    "                if token_info[5]:\n",
    "                    golden_dict[token_info[0]]['num'] = token_info[5]\n",
    "                if token_info[6]:\n",
    "                    golden_dict[token_info[0]]['tense'] = token_info[6]\n",
    "                if token_info[7]:\n",
    "                    golden_dict[token_info[0]]['mood'] = token_info[7]\n",
    "                if token_info[8]:\n",
    "                    golden_dict[token_info[0]]['inf'] = token_info[8]\n",
    "                if token_info[9]:\n",
    "                    golden_dict[token_info[0]]['partcp'] = token_info[9]\n",
    "                if token_info[10]:\n",
    "                    golden_dict[token_info[0]]['ger'] = token_info[10]\n",
    "                if token_info[11]:\n",
    "                    golden_dict[token_info[0]]['voice'] = token_info[11]\n",
    "                if token_info[12]:\n",
    "                    golden_dict[token_info[0]]['person'] = token_info[12]\n",
    "                \n",
    "    return golden_dict        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим accuracy для каждого теггера.\n",
    "\n",
    "**Accuracy = P/N**\n",
    "\n",
    "где P – количество токенов по которым классификатор принял правильное решение, а N – размер обучающей выборки (количество токенок в золотом стандарте). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "pymorphy_dict = pymorphy_analysis(text)\n",
    "slovnet_dict = slovnet_analysis(text)\n",
    "udpipe_dict = udpipe_analysis(text)\n",
    "golden_dict = get_golden_standart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(morph_dict, golden_dict):\n",
    "    tokens = list(map(lambda x: x.lower(), gold_dict.keys()))\n",
    "    morph_answers = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            if morph_dict[token]['POS'] == golden_dict[token]['POS']:\n",
    "                morph_answers.append(1)\n",
    "            else:\n",
    "                morph_answers.append(0)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "            \n",
    "    return round(sum(morph_answers)/len(gold_dict), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy для pymorphy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.789"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(pymorphy_dict, golden_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy для Natasha**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.766"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(slovnet_dict, golden_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy для Udpipe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.766"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(udpipe_dict, golden_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выигрывает ✨ Pymorphy ✨**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chunker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше вам нужно взять лучший теггер для русского языка (aka Pymorphy) и с его помощью написать функцию (chunker), которая выделяет из размеченного текста 3 типа n-грамм, соответствующих какому-то шаблону (к примеру не + какая-то часть речи или NP или сущ.+ наречие и тд)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 типа n-грамм:**\n",
    "- 'не' + VERB\n",
    "- ADJ + NOUN\n",
    "- VERB + (ADJ.Acc)? + NOUN.Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pattern_NEG_any_POS(tokens):\n",
    "    pattern_matches = []\n",
    "    \n",
    "    for i in range(len(tokens)-1):\n",
    "        current_token = tokens[i].lower()\n",
    "        next_token = tokens[i+1].lower()\n",
    "        if current_token == 'не':\n",
    "            pattern_matches.append((current_token, next_token))\n",
    "    \n",
    "    return pattern_matches     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('не', 'только'), ('не', 'приходится'), ('не', 'могла'), ('не', 'полюбить')]"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pattern_NEG_any_POS(tokenizer(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pattern_ADJ_NOUN(tokens):\n",
    "    pattern_matches = []\n",
    "\n",
    "    for i in range(len(tokens)-1):\n",
    "        current_token = tokens[i].lower()\n",
    "        next_token = tokens[i+1].lower()\n",
    "        current_token_POS = pymorph.parse(current_token)[0].tag.POS\n",
    "        next_token_POS = pymorph.parse(next_token)[0].tag.POS\n",
    "        if (current_token_POS == 'ADJF' or current_token_POS == 'ADJS') and next_token_POS == 'NOUN':\n",
    "            pattern_matches.append((current_token, next_token))\n",
    "    return pattern_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('осознанным', 'покупателем'),\n",
       " ('углеродный', 'след'),\n",
       " ('своего', 'гардероба'),\n",
       " ('конкретной', 'вещи'),\n",
       " ('независимые', 'марки'),\n",
       " ('самом', 'начале'),\n",
       " ('кодовым', 'названием'),\n",
       " ('желтый', 'жакет'),\n",
       " ('своей', 'страстью'),\n",
       " ('винтажной', 'стилистике'),\n",
       " ('броских', 'брюках'),\n",
       " ('очередное', 'подтверждение'),\n",
       " ('экологичные', 'творения'),\n",
       " ('карантинный', 'краш'),\n",
       " ('лос-анджелесской', 'художницы'),\n",
       " ('старые', 'пары'),\n",
       " ('уникальные', 'вещи'),\n",
       " ('экологичной', 'репутацией'),\n",
       " ('ультракороткие', 'юбки'),\n",
       " ('шиммерные', 'тени'),\n",
       " ('большая', 'поклонница'),\n",
       " ('старые', 'джинсы')]"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pattern_ADJ_NOUN(tokenizer(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pattern_VERB_NOUN_Acc(tokens):\n",
    "    pattern_matches = []\n",
    "\n",
    "    for i in range(len(tokens)-2):\n",
    "        current_token = tokens[i].lower()\n",
    "        next_token = tokens[i+1].lower()\n",
    "        \n",
    "        current_token_POS = pymorph.parse(current_token)[0].tag.POS\n",
    "        next_token_POS = pymorph.parse(next_token)[0].tag.POS\n",
    "        next_token_case = pymorph.parse(next_token)[0].tag.case\n",
    "        \n",
    "        if current_token_POS == 'VERB' and next_token_POS == 'NOUN' and next_token_case == 'accs':\n",
    "            pattern_matches.append((current_token, next_token))\n",
    "    return pattern_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('продлеваете', 'жизнь'), ('заполонила', 'ленту')]"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pattern_VERB_NOUN_Acc(tokenizer(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(text):\n",
    "    tokens = tokenizer(text)\n",
    "    pattern_NEG_VERB = get_pattern_NEG_VERB(tokens)\n",
    "    patter_ADJ_NOUN = get_pattern_ADJ_NOUN(tokens)\n",
    "    pattern_VERB_NOUN_Acc = get_pattern_VERB_NOUN_Acc(tokens)\n",
    "    \n",
    "    return pattern_NEG_VERB, patter_ADJ_NOUN, pattern_VERB_NOUN_Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('не', 'приходится'), ('не', 'могла')],\n",
       " [('осознанным', 'покупателем'),\n",
       "  ('углеродный', 'след'),\n",
       "  ('своего', 'гардероба'),\n",
       "  ('конкретной', 'вещи'),\n",
       "  ('независимые', 'марки'),\n",
       "  ('самом', 'начале'),\n",
       "  ('кодовым', 'названием'),\n",
       "  ('желтый', 'жакет'),\n",
       "  ('своей', 'страстью'),\n",
       "  ('винтажной', 'стилистике'),\n",
       "  ('броских', 'брюках'),\n",
       "  ('очередное', 'подтверждение'),\n",
       "  ('экологичные', 'творения'),\n",
       "  ('карантинный', 'краш'),\n",
       "  ('лос-анджелесской', 'художницы'),\n",
       "  ('старые', 'пары'),\n",
       "  ('уникальные', 'вещи'),\n",
       "  ('экологичной', 'репутацией'),\n",
       "  ('ультракороткие', 'юбки'),\n",
       "  ('шиммерные', 'тени'),\n",
       "  ('большая', 'поклонница'),\n",
       "  ('старые', 'джинсы')],\n",
       " [('продлеваете', 'жизнь'), ('заполонила', 'ленту')])"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunker(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Улучшаем прошлую домашку"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предложите 3 шаблона (слово + POS-тег / POS-тег + POS-тег) запись которых в словарь, по вашему мнению, улучшила бы качество работы программы из предыдущей домашки. \n",
    "\n",
    "✅ Почему именно эти группы вы взяли.\n",
    "\n",
    "✅ Встраивание функции в программу из предыдущей домашки.\n",
    "\n",
    "✅ Сравнение качества предсказания тональности с улучшением и без (это бонусный одиннадцатый балл)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 шаблона:**\n",
    "- 'не' + VERB \n",
    "- ADV + VERB \n",
    "- ADJ + NOUN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Почему именно эти шаблоны?**\n",
    "_Не_,  наречие перед глаголом и прилагательное перед существительным существенно влияют на смысл словосочетания. \n",
    "- Сможем различать _не понравилось / понравилось_ и _не разочаровали / разочаровали_.\n",
    "- Сможем различать _ужасно понравилось_ и _ужасно разочаровались_.\n",
    "- Сможем различать _великолепный отель_ и _великолепная пыль_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Функции для выделения паттернов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pattern_NEG_VERB(tokens):\n",
    "    pattern_matches = []\n",
    "    \n",
    "    for i in range(len(tokens)-1):\n",
    "        current_token = tokens[i].lower()\n",
    "        next_token = tokens[i+1].lower()\n",
    "        if current_token == 'не' and pymorph.parse(next_token)[0].tag.POS in ['INFN', 'GRND', 'PRTF', 'PRTS']:\n",
    "            pattern_matches.append((current_token, next_token))\n",
    "    \n",
    "    return pattern_matches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('не', 'полюбить')]"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Паттерн ADV + VERB (_очень понравилось, ужасно разочаровались_)\n",
    "get_pattern_NEG_VERB(tokenizer(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pattern_ADV_VERB(tokens):\n",
    "    pattern_matches = []\n",
    "\n",
    "    for i in range(len(tokens)-2):\n",
    "        current_token = tokens[i].lower()\n",
    "        next_token = tokens[i+1].lower()\n",
    "        \n",
    "        current_token_POS = pymorph.parse(current_token)[0].tag.POS\n",
    "        next_token_POS = pymorph.parse(next_token)[0].tag.POS\n",
    "        \n",
    "        if current_token_POS == 'ADVB' and next_token_POS == 'VERB':\n",
    "            pattern_matches.append((current_token, next_token))\n",
    "    return pattern_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('еще', 'продлеваете'),\n",
       " ('постоянно', 'поддерживает'),\n",
       " ('буквально', 'заполонила')]"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Паттерн ADV + VERB (_очень понравилось, ужасно разочаровались_)\n",
    "get_pattern_ADV_VERB(tokenizer(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('осознанным', 'покупателем'),\n",
       " ('углеродный', 'след'),\n",
       " ('своего', 'гардероба'),\n",
       " ('конкретной', 'вещи'),\n",
       " ('независимые', 'марки'),\n",
       " ('самом', 'начале'),\n",
       " ('кодовым', 'названием'),\n",
       " ('желтый', 'жакет'),\n",
       " ('своей', 'страстью'),\n",
       " ('винтажной', 'стилистике'),\n",
       " ('броских', 'брюках'),\n",
       " ('очередное', 'подтверждение'),\n",
       " ('экологичные', 'творения'),\n",
       " ('карантинный', 'краш'),\n",
       " ('лос-анджелесской', 'художницы'),\n",
       " ('старые', 'пары'),\n",
       " ('уникальные', 'вещи'),\n",
       " ('экологичной', 'репутацией'),\n",
       " ('ультракороткие', 'юбки'),\n",
       " ('шиммерные', 'тени'),\n",
       " ('большая', 'поклонница'),\n",
       " ('старые', 'джинсы')]"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Паттерн ADJ + NOUN (_великолепный отель, великолепная пыль_)\n",
    "get_pattern_ADJ_NOUN(tokenizer(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Встраиваю функции в прошлую домашку"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "См. [HW_1.ipynb](https://github.com/soimmary/NLP_HW/blob/main/HW_1/HW_1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Спойлер**: accuracy определения положительных отзывов увеличилось в 2 раза!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
